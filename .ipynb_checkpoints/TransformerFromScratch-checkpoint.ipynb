{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214a1356-2be9-4f8b-b46a-d0b246a99eb3",
   "metadata": {},
   "source": [
    "# Building a Music Transformer from the ground up with PyTorch\n",
    "This is a follow up to a project I've already completed building a simple feed-forward neural network using numpy. Since Transformers are all the rage right now and I have some familiarity with PyTorch as a library, this seemed like a great project to improve that knowledge and also get to grips with the fundamental theory and mathematics behind LLMs. Also, this is my first Jupyter Notebook which seemed like another tool that would be good to pick up.\n",
    "\n",
    "The plan:\n",
    "- Learn the theory and maths behind each block from articles and the original Google paper\n",
    "- Note down my understanding of the methods and maths here\n",
    "- Build a transformer from the ground up\n",
    "-Make it play music!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce98506-7c11-4a01-b48e-34c3a52eeb8b",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*8698hoEFnRuNtQ7vT8Lm1A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60df52-6daa-4f82-88a1-5cbaa89e088d",
   "metadata": {},
   "source": [
    "### Why use a Transformer instead of an RNN?\n",
    "- RNNs run very slowly for long sequences of tokens\n",
    "- Using lots of hidden layers in an RNN can cause long gradient calculations to either vanish or explode due to limited precision of number representation in computation. This cause either very small or very large training updates, leading to errors\n",
    "- Due to the long chain of dependencies in an RNN the \"effect\" of the first token in a sequence diminishes, meaning the model cannot maintain \"long-range dependencies\" i.e it struggles to infer context in very long sequences of text\n",
    "\n",
    "Transformers solve all of these issues!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d6e86-0769-453f-aaa5-76988ac5dea8",
   "metadata": {},
   "source": [
    "### The Encoder:\n",
    "Transformers are split into two parts: the Encoder and Decoder which run in parallel, First up is the encoder:\n",
    "The encoder produces **keys** and **values** for the encoder's multi-head attention block.\n",
    "![](https://miro.medium.com/v2/resize:fit:524/format:webp/1*No33bjhlMKb0-IUqQAGH9g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cf70b-31e4-4525-9820-8a011e71215c",
   "metadata": {},
   "source": [
    "#### Input Embeddings:\n",
    "Given a sequence of words, the sequence is first tokenized (perhaps just separating into individual words), and these tokens are then associated with a token ID (representative of their position in the vocabulary). After this the token ID is converted into a vector **emmbedding** (in this case we are using vectors of size 512). \n",
    "Note these embeddings are not fixed and in fact will be altered during the training process (this is how a transformer appears to process \"meaning\", or different characteristics of these words), however the token IDs are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92f6d5ce-33bd-4827-ba53-14490b2a7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(InputEmbeddings, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)  # use nn.Embedding to get the word embeddings from pytorch\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e978ea3-5c52-4576-9a55-77e8f53046c5",
   "metadata": {},
   "source": [
    "#### Positional Encoding:\n",
    "The next step is positional encoding. While the embeddings aim to capture meaning, this step aims to make the model understand that words near one another are related (i.e adjective noun). This helps the model to recognise patterns in sentence structure. The formulae for positional encoding are as follows (taken directly from the original Transformer paper by Google):\n",
    "\n",
    "$ PE(pos, 2i) = sin{\\frac{pos}{1000^{\\frac{2i}{d}}}} $\n",
    "\n",
    "$ PE(pos, 2i + 1) = cos{\\frac{pos}{1000^{\\frac{2i}{d}}}}$\n",
    "\n",
    "Whereby d is the dimension of the model, pos is the position of the token in the sequence, and i is the row of the embedding we are currently processing.\n",
    "This means the first formula is applied to the even rows, and the second to the odd rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d4fef-7216-43ee-be88-83c2b9ab1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #create a zero tensor to fill in the positional embeddings\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #create positions vector (note arange creates a vector that steps by 1 at each entry so perfect for this use case)\n",
    "        position = torch.arange(start = 0, end = seq_len, dtype = torch.float).unsqueeze(1)   #this makes a vector (0.0, 1.1, 2.2 ... seq_length), then unsqueezes it to be (seq_length, 1)\n",
    "        #create a \"division vector\" to speed up calculations of the above formulae\n",
    "        div_term = torch.arange(0, d_model, 2).float() * (-math.log(1000)/d_model)     #this arange has a step of 2, baking in the 1000^2i/d here\n",
    "        #apply the formulae\n",
    "        pe[:,0::2] = torch.sin(position * div_term)   #note the [:,0::2 or 1] notation hits the even and odd terms of the pe vector respectively\n",
    "        pe[:,0::1] = torch.cos(position * div_term)\n",
    "\n",
    "        #add an extra dimension to make this applicable to batching\n",
    "        pe = pe.unsqeeze(0)\n",
    "\n",
    "        #want to make the module \"remember\" these embeddings so call the buffer method\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is of shape [batch, pe, dim] so want to acces x(1) to get the dim of pes we require\n",
    "        #from the paper the way we use positional embeddings is to add them to the original embeddings\n",
    "        x = x + (self.pe[:, :x.shape(1), :]).requiresgrad(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f657c95-0e98-41b4-b4db-5684d0c4a32c",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "In order to understand multi-head attention it is first helpful to understand **self-attention**, which the researchers at google adapted into multi-head attention.\n",
    "Attention essentially calculates a score for each pair of words (outputted as a matrix of the same dimension as the original embedding) which can be thought of as the strength of relation of one word to another, by doing:\n",
    "\n",
    "$ Attention(Q,K,V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V $\n",
    "\n",
    "Whereby Q,K,V are all calls to the respective input matrix Q(query), K(key), V(values)\n",
    "\n",
    "Self-attention is permutation invariant, we expect the diagonal entries of the permutation matrix to be the largest, and certain positions can be set to $-\\infty$ if we don't want them to interact\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*3d9DWq5s-36cU1kbN6VJdA.png)\n",
    "\n",
    "Multi-head attention is different. The steps are as follows:\n",
    "- Copy the input embeddings to make three copies $ Q, K, V $    (dim = (seq, d_model))\n",
    "- Multiply by weight matrices $ W^Q, W^K, W^V $    (these can be tuned during training)\n",
    "- Label the results $ Q', K', V' $\n",
    "- Split this into h new matrices where h is the number of heads we desire (in the diagram h = 4)\n",
    "- We then apply the self-attention formula to each head as above\n",
    "- Label the resultant matrices $ head1, head2, head3 $; they will have dim = (seq, d_k) (d_k = d_model/h)\n",
    "- This is then multiplied by a final weight matrix with dim = (h*d_k, d_model) to get the output matrix with the same size as the input\n",
    "\n",
    "Each head can be thought of as a different aspect of a word. For example head1 could learn to relate the word as a noun, head2 as an adjective, head3 a verb and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5b3e6-cd7b-4743-9cb6-08d5d2016a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        assert d_model % n_heads == 0, \"Division into n_heads must be possible\"\n",
    "        self.d_k = d_model // n_heads   #d_k is the dim of the tensors that are run through the heads\n",
    "\n",
    "        #initialise weight matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias = False)  #by using a Linear layer we can speed up the calculations via PyTorch, and setting bais to False makes this just a weights matrix as we want\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias = False)\n",
    "\n",
    "        #initialise W_0:\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        @staticmethod   #this means we're defining a function that doesn't need to modify the class state\n",
    "        def attention (query, key, values, mask, dropout: nn.Dropout):\n",
    "            d_k = query.shape[-1]\n",
    "            #Transform: [Batch, n_heads, seq_len, d_k] -> [Batch, n_heads, seq_len, seq_len]\n",
    "            attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)  #@ operator performs matmul so this is application of the self-attention formula\n",
    "            attention_scores = attention_scores.softmax(dim = -1)\n",
    "\n",
    "            if mask is not None:\n",
    "                attention_scores.masked_fill(mask == 0, -1e9) #replaces all elements where the mask has a 0 with -1e9\n",
    "\n",
    "            if dropout is not None:\n",
    "                attention_scores = dropout(attention_scores)   #just calling the PyTorch method to apply dropout here\n",
    "\n",
    "            return (attention_scores @ value), attention_scores   #again using @ for matmul\n",
    "\n",
    "\n",
    "        def forward(self, q, k, v, mask):\n",
    "            #project embeddings into weight matrices:\n",
    "            query = self.w_q(q)\n",
    "            key = self.w_k(k)\n",
    "            values = self.w_v(v)\n",
    "\n",
    "            #need to transpose from [batch, seq_len, d_model] to [batch, seq_len, n_heads, d_k] to [batch, n_heads, seq_len, d_k]\n",
    "            query = query.view(query.shape[0], query.shape[1], self.n_heads, self.d_k).transpose(1, 2)  \n",
    "            key = key.view(key.shape[0], key.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "            value = value.view(value.shape[0], value.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "            #time to run attention on each\n",
    "            x, self.attention_scores = MultiHeadAttention.attention(query, key, values, mask, self.dropout)\n",
    "            \n",
    "            #transform the output to [batch_size, seq_len, n_heads * d_k] = [batch_size, seq_len, d_model]\n",
    "            x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.n_heads * self.d_k)\n",
    "\n",
    "            return self.w_0(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056fac2-1876-4313-8bc7-e229c63cc533",
   "metadata": {},
   "source": [
    "#### ADD and Norm (Layer Normalization)\n",
    "Given a batch of n items which all have features that could be embedded, follow these steps:\n",
    "- Calculate the mean $\\mu$ and variance $\\sigma ^2$ of each item independently\n",
    "- Adjust each $x_i$ in the embedding by using the following formula:\n",
    "  $x_i = \\frac{x_i - \\mu _i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$\n",
    "- This is then multiplied by paramaters $\\alpha$, $\\lambda$(multiplicative) or $\\beta$(additive)\n",
    "- $\\epsilon$ is added in the denominator so it doesn't approach zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e7dde-176e-45a0-9bef-919bb25f0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps = 10e-6):\n",
    "        self.eps = eps\n",
    "        #want alpha and beta to be trainable so we call the Paramater method which tells PyTorch to train these\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))  #both of these create a 1d tensor with 1 element i.e [0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)   #-1 is the last dimension, makes sure we are taking the mean of the values of x\n",
    "        std = x.std(dime = -1, keepdim = True)\n",
    "        norm = self.alpha * (x -mean)/math.sqrt(std + eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd9037-b257-4dba-99e2-bc6369e93f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a \"connection\" layer that applies the normalization step and connects the other blocks to allow for faster training\n",
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalization = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        #Normalize x, then pass it through a sublayer (any type), use the dropout term, and finally add x\n",
    "        return x + self.dropout(sublayer(self.normalization(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61961c93-d30e-4066-a712-d46148b2cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to stack multiple encoders after embedding we create a block that does attention, normalization, feed forward layers\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__ (self, attention_block :MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attention_block = attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])  #creates a list with 2 residual connection modules\n",
    "\n",
    "    def forward(self, x ,mask):\n",
    "        x = self.residual_connections[0](lambda x: self.attention_block(x, x, x, mask)  #first connection block takes output of multi-head attention so we feed x into attention here\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block(x))  #second one feeds into the feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea078b-6185-46f4-a93f-c7acbc8e0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now time for the main encoder class:\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers: nn.ModuleList):\n",
    "        super().__init()\n",
    "        self.n_layers = n_layers\n",
    "        self.normalization = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.n_layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.normalization(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5a1cc-cf75-45af-b8c7-4cdbb27bad58",
   "metadata": {},
   "source": [
    "### The Decoder:\n",
    "That completes the encoder, now it's time to build the decoder:\n",
    "![](https://miro.medium.com/v2/resize:fit:544/format:webp/1*Hjin7_ljwxRcmvojICizig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c3d94-4721-4c2c-8dba-61a16f3b7942",
   "metadata": {},
   "source": [
    "#### Masked Multi-Head Attention\n",
    "The idea here is to make the model **casual**, meaning the output at a given position only depends on the preceding words. This means we have to stop the transformer seeing future words, which is achieved my **masking** during the multi-head attention process.\n",
    "Specifically this involves setting all entries above the diagonal to $-\\infty$.\n",
    "This block then produces the **querys** for the decoder's main multi-head attention block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e0c9d-1540-44a9-aa2f-15764f342767",
   "metadata": {},
   "source": [
    "#### Feed Forward\n",
    "The output of the multi-head attention block followed by ADD-and-norm is a tensor which can be fed into a standard feed-forward neural network. This usually consists of two fully connected layers with ReLU activation functions to allow the model to learn non-linear behaviour. The output is always a tensor of the same shape as the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f93d39-2940-4f0a-b4a4-1757b0cc30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    #this is just a standard linear model like i've built plenty of times before\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(nn.ReLU(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13d42a-f3a1-4899-b7f5-892ab10297d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same idea with Decoder; want to be able to stack multiple after embedding so we define an attention-norm-feedforward block first\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention: MultiHeadAttention, cross_attention: MultiHeadAttention, feed_forward: FeedForwardBlock, dropout: float):\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])  #creates a list with 3 residual connection modules\n",
    "\n",
    "    def forward(self, x, encoder_out, mask1, mask2):\n",
    "        x = self.residual_connections[0](lambda x: self.self_attention(x, x, x, mask1))\n",
    "        x = self.residual_connections[1](lambda x: self.cross_attention(x, encoder_out, encoder_out, mask2))\n",
    "        x = self.residual_connections[2](x, self.feed_forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd32a6-a140-4e81-95d4-d385e602a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers: nn.ModuleList):\n",
    "        self.n_layers = n_layers\n",
    "        self.normalization = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_out, mask1, mask2):\n",
    "        for layer in self.n_layers:\n",
    "            x = layer(x, encoder_out, mask1, mask2)\n",
    "        return self.normalization(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4da5b-6651-4f8d-bd90-7c9c3c951d3a",
   "metadata": {},
   "source": [
    "#### Output\n",
    "This is just a standard linear layer with softmax activation like you would use in a classifier NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d2292-bf0b-4df7-bfa7-1ea20d1fe975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLinear(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log.softmax(self.fc(x), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d130f4f-54e5-4d4d-b9d6-6fcd1b5c2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, \n",
    "                 tgt_embed: InputEmbeddings, src_pos: PositionalEmbeddings, tgt_pos: PositionalEmbeddings, \n",
    "                 last_linear: LastLinear):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.last_linear = last_linear\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, enc_out, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "\n",
    "    def linear(self, x):\n",
    "        return self.last_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120297a-4d19-40be-86a0-9dfccbfe82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len:int, d_model: int = 512, n_layers: int = 6,\n",
    "                      n_heads:int = 8, dropout: float = 0.1, hidden_size: int = 2048):\n",
    "    #first step is to make embedding layers:\n",
    "    src_embeddings = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embeddings = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    #now we make pos embed layers:\n",
    "    src_pos = PositionalEmbeddings(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEmbeddings(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    #create the encoder blocks:\n",
    "    encoder_blocks = []\n",
    "    for _ in range(n_layers):\n",
    "        encoder_self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        feed_forward = FeedForwardBlock(d_model, hidden_size, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention, feed_forward, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    #create the decoder blocks:\n",
    "    decoder_blocks = []\n",
    "    for _ in range(n_layers):\n",
    "        decoder_self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        decoder_cross_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        feed_forward = FeedForwardBlock(d_model, hidden_size, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention, decoder_cross_attention, feed_forward, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    #create encoder:\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "\n",
    "    #create decoder:\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    #create last linear layers\n",
    "    last_layer = LastLinear(d_model, tgt_vocab_size)\n",
    "\n",
    "    #finally time to define the Transformer in full:\n",
    "    transformer = Transformer(encoder, decoder, src_embeddings, tgt_embeddings, src_pos, tgt_pos, last_layer)\n",
    "\n",
    "    #initialise paramaters with Xavier intialisation (helps to avoid vanishing gradients)    \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)    \n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff10e39-7804-454b-af27-42d3e72506b4",
   "metadata": {},
   "source": [
    "### Training\n",
    "Before implementing a Music Transformer, I though it prudent to train this one to do machine translation as it was originally developed for by google. To do this I will use the multi30k dataset from torchtext, and pyTorch to implement tokenisation, batching etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92eb5a-ae7c-4239-bc1f-16305987de96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2e546-a0ff-4548-b13c-ba0f336b6745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
