{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214a1356-2be9-4f8b-b46a-d0b246a99eb3",
   "metadata": {},
   "source": [
    "# Building a Music Transformer from the ground up with PyTorch\n",
    "This is a follow up to a project I've already completed building a simple feed-forward neural network using numpy. Since Transformers are all the rage right now and I have some familiarity with PyTorch as a library, this seemed like a great project to improve that knowledge and also get to grips with the fundamental theory and mathematics behind LLMs. Also, this is my first Jupyter Notebook which seemed like another tool that would be good to pick up.\n",
    "\n",
    "The plan:\n",
    "- Learn the theory and maths behind each block from articles and the original Google paper\n",
    "- Note down my understanding of the methods and maths here\n",
    "- Build a transformer from the ground up\n",
    "-Make it play music!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce98506-7c11-4a01-b48e-34c3a52eeb8b",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*8698hoEFnRuNtQ7vT8Lm1A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60df52-6daa-4f82-88a1-5cbaa89e088d",
   "metadata": {},
   "source": [
    "### Why use a Transformer instead of an RNN?\n",
    "- RNNs run very slowly for long sequences of tokens\n",
    "- Using lots of hidden layers in an RNN can cause long gradient calculations to either vanish or explode due to limited precision of number representation in computation. This cause either very small or very large training updates, leading to errors\n",
    "- Due to the long chain of dependencies in an RNN the \"effect\" of the first token in a sequence diminishes, meaning the model cannot maintain \"long-range dependencies\" i.e it struggles to infer context in very long sequences of text\n",
    "\n",
    "Transformers solve all of these issues!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d6e86-0769-453f-aaa5-76988ac5dea8",
   "metadata": {},
   "source": [
    "### The Encoder:\n",
    "Transformers are split into two parts: the Encoder and Decoder which run in parallel, First up is the encoder:\n",
    "The encoder produces **keys** and **values** for the encoder's multi-head attention block.\n",
    "![](https://miro.medium.com/v2/resize:fit:524/format:webp/1*No33bjhlMKb0-IUqQAGH9g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cf70b-31e4-4525-9820-8a011e71215c",
   "metadata": {},
   "source": [
    "#### Input Embeddings:\n",
    "Given a sequence of words, the sequence is first tokenized (perhaps just separating into individual words), and these tokens are then associated with a token ID (representative of their position in the vocabulary). After this the token ID is converted into a vector **emmbedding** (in this case we are using vectors of size 512). \n",
    "Note these embeddings are not fixed and in fact will be altered during the training process (this is how a transformer appears to process \"meaning\", or different characteristics of these words), however the token IDs are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92f6d5ce-33bd-4827-ba53-14490b2a7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(InputEmbeddings, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)  # use nn.Embedding to get the word embeddings from pytorch\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e978ea3-5c52-4576-9a55-77e8f53046c5",
   "metadata": {},
   "source": [
    "#### Positional Encoding:\n",
    "The next step is positional encoding. While the embeddings aim to capture meaning, this step aims to make the model understand that words near one another are related (i.e adjective noun). This helps the model to recognise patterns in sentence structure. The formulae for positional encoding are as follows (taken directly from the original Transformer paper by Google):\n",
    "\n",
    "$ PE(pos, 2i) = sin{\\frac{pos}{1000^{\\frac{2i}{d}}}} $\n",
    "\n",
    "$ PE(pos, 2i + 1) = cos{\\frac{pos}{1000^{\\frac{2i}{d}}}}$\n",
    "\n",
    "Whereby d is the dimension of the model, pos is the position of the token in the sequence, and i is the row of the embedding we are currently processing.\n",
    "This means the first formula is applied to the even rows, and the second to the odd rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d4fef-7216-43ee-be88-83c2b9ab1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #create a zero tensor to fill in the positional embeddings\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #create positions vector (note arange creates a vector that steps by 1 at each entry so perfect for this use case)\n",
    "        position = torch.arange(start = 0, end = seq_len, dtype = torch.float).unsqueeze(1)   #this makes a vector (0.0, 1.1, 2.2 ... seq_length), then unsqueezes it to be (seq_length, 1)\n",
    "        #create a \"division vector\" to speed up calculations of the above formulae\n",
    "        div_term = torch.arange(0, d_model, 2).float() * (-math.log(1000)/d_model)     #this arange has a step of 2, baking in the 1000^2i/d here\n",
    "        #apply the formulae\n",
    "        pe[:,0::2] = torch.sin(position * div_term)   #note the [:,0::2 or 1] notation hits the even and odd terms of the pe vector respectively\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        #add an extra dimension to make this applicable to batching\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        #want to make the module \"remember\" these embeddings so call the buffer method\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is of shape [batch, pe, dim] so want to acces x(1) to get the dim of pes we require\n",
    "        #from the paper the way we use positional embeddings is to add them to the original embeddings\n",
    "        x = x + self.pe[:, :x.shape[1], :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f657c95-0e98-41b4-b4db-5684d0c4a32c",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "In order to understand multi-head attention it is first helpful to understand **self-attention**, which the researchers at google adapted into multi-head attention.\n",
    "Attention essentially calculates a score for each pair of words (outputted as a matrix of the same dimension as the original embedding) which can be thought of as the strength of relation of one word to another, by doing:\n",
    "\n",
    "$ Attention(Q,K,V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V $\n",
    "\n",
    "Whereby Q,K,V are all calls to the respective input matrix Q(query), K(key), V(values)\n",
    "\n",
    "Self-attention is permutation invariant, we expect the diagonal entries of the permutation matrix to be the largest, and certain positions can be set to $-\\infty$ if we don't want them to interact\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*3d9DWq5s-36cU1kbN6VJdA.png)\n",
    "\n",
    "Multi-head attention is different. The steps are as follows:\n",
    "- Copy the input embeddings to make three copies $ Q, K, V $    (dim = (seq, d_model))\n",
    "- Multiply by weight matrices $ W^Q, W^K, W^V $    (these can be tuned during training)\n",
    "- Label the results $ Q', K', V' $\n",
    "- Split this into h new matrices where h is the number of heads we desire (in the diagram h = 4)\n",
    "- We then apply the self-attention formula to each head as above\n",
    "- Label the resultant matrices $ head1, head2, head3 $; they will have dim = (seq, d_k) (d_k = d_model/h)\n",
    "- This is then multiplied by a final weight matrix with dim = (h*d_k, d_model) to get the output matrix with the same size as the input\n",
    "\n",
    "Each head can be thought of as a different aspect of a word. For example head1 could learn to relate the word as a noun, head2 as an adjective, head3 a verb and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5b3e6-cd7b-4743-9cb6-08d5d2016a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        assert d_model % n_heads == 0, \"Division into n_heads must be possible\"\n",
    "        self.d_k = d_model // n_heads   #d_k is the dim of the tensors that are run through the heads\n",
    "\n",
    "        #initialise weight matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias = False)  #by using a Linear layer we can speed up the calculations via PyTorch, and setting bais to False makes this just a weights matrix as we want\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias = False)\n",
    "\n",
    "        #initialise W_0:\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod   #this means we're defining a function that doesn't need to modify the class state\n",
    "    def attention (query, key, values, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        #Transform: [Batch, n_heads, seq_len, d_k] -> [Batch, n_heads, seq_len, seq_len]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)  #@ operator performs matmul so this is application of the self-attention formula\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask == 0, -1e9) #replaces all elements where the mask has a 0 with -1e9\n",
    "\n",
    "        attention_scores = attention_scores.softmax(dim = -1) #apply softmax to the last dimension\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)   #just calling the PyTorch method to apply dropout here\n",
    "\n",
    "        return (attention_scores @ values), attention_scores   #again using @ for matmul\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "    #project embeddings into weight matrices:\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        values = self.w_v(v)\n",
    "\n",
    "        #need to transpose from [batch, seq_len, d_model] to [batch, seq_len, n_heads, d_k] to [batch, n_heads, seq_len, d_k]\n",
    "        query = query.view(query.shape[0], query.shape[1], self.n_heads, self.d_k).transpose(1, 2)  \n",
    "        key = key.view(key.shape[0], key.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        values = values.view(values.shape[0], values.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        #time to run attention on each\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, values, mask, self.dropout)\n",
    "            \n",
    "        #transform the output to [batch_size, seq_len, n_heads * d_k] = [batch_size, seq_len, d_model]\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.n_heads * self.d_k)\n",
    "\n",
    "        return self.w_0(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056fac2-1876-4313-8bc7-e229c63cc533",
   "metadata": {},
   "source": [
    "#### ADD and Norm (Layer Normalization)\n",
    "Given a batch of n items which all have features that could be embedded, follow these steps:\n",
    "- Calculate the mean $\\mu$ and variance $\\sigma ^2$ of each item independently\n",
    "- Adjust each $x_i$ in the embedding by using the following formula:\n",
    "  $x_i = \\frac{x_i - \\mu _i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$\n",
    "- This is then multiplied by paramaters $\\alpha$, $\\lambda$(multiplicative) or $\\beta$(additive)\n",
    "- $\\epsilon$ is added in the denominator so it doesn't approach zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e7dde-176e-45a0-9bef-919bb25f0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps = 10e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        #want alpha and beta to be trainable so we call the Paramater method which tells PyTorch to train these\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))  #both of these create a 1d tensor with 1 element i.e [0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)   #-1 is the last dimension, makes sure we are taking the mean of the values of x\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        norm = self.alpha * (x -mean)/torch.sqrt(std ** 2 + self.eps) + self.beta\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd9037-b257-4dba-99e2-bc6369e93f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a \"connection\" layer that applies the normalization step and connects the other blocks to allow for faster training\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalization = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        #Normalize x, then pass it through a sublayer (any type), use the dropout term, and finally add x\n",
    "        return x + self.dropout(sublayer(self.normalization(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61961c93-d30e-4066-a712-d46148b2cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to be able to stack multiple blocks, need to build a block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__ (self, attention_block :MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attention_block = attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])  #creates a list with 2 residual connection modules\n",
    "\n",
    "    def forward(self, x ,mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.attention_block(x, x, x, mask))  #first connection block takes output of multi-head attention so we feed x into attention here\n",
    "        x = self.residual_connections[1](x, lambda x: self.feed_forward_block(x))  #second one feeds into the feed forward layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea078b-6185-46f4-a93f-c7acbc8e0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now time for the main encoder class:\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.normalization = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.n_layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.normalization(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5a1cc-cf75-45af-b8c7-4cdbb27bad58",
   "metadata": {},
   "source": [
    "### The Decoder:\n",
    "That completes the encoder, now it's time to build the decoder:\n",
    "![](https://miro.medium.com/v2/resize:fit:544/format:webp/1*Hjin7_ljwxRcmvojICizig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c3d94-4721-4c2c-8dba-61a16f3b7942",
   "metadata": {},
   "source": [
    "#### Masked Multi-Head Attention\n",
    "The idea here is to make the model **casual**, meaning the output at a given position only depends on the preceding words. This means we have to stop the transformer seeing future words, which is achieved my **masking** during the multi-head attention process.\n",
    "Specifically this involves setting all entries above the diagonal to $-\\infty$.\n",
    "This block then produces the **querys** for the decoder's main multi-head attention block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e0c9d-1540-44a9-aa2f-15764f342767",
   "metadata": {},
   "source": [
    "#### Feed Forward\n",
    "The output of the multi-head attention block followed by ADD-and-norm is a tensor which can be fed into a standard feed-forward neural network. This usually consists of two fully connected layers with ReLU activation functions to allow the model to learn non-linear behaviour. The output is always a tensor of the same shape as the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f93d39-2940-4f0a-b4a4-1757b0cc30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    #this is just a standard linear model like i've built plenty of times before\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13d42a-f3a1-4899-b7f5-892ab10297d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same idea with Decoder; want to be able to stack multiple after embedding so we define an attention-norm-feedforward block first\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention: MultiHeadAttention, cross_attention: MultiHeadAttention, feed_forward: FeedForwardBlock, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])  #creates a list with 3 residual connection modules\n",
    "\n",
    "    def forward(self, x, encoder_out, mask1, mask2):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention(x, x, x, mask1))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention(x, encoder_out, encoder_out, mask2))\n",
    "        x = self.residual_connections[2](x, lambda x: self.feed_forward(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd32a6-a140-4e81-95d4-d385e602a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.normalization = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_out, tgt_mask, src_mask):\n",
    "        for layer in self.n_layers:\n",
    "            x = layer(x, encoder_out, tgt_mask, src_mask)\n",
    "        return self.normalization(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4da5b-6651-4f8d-bd90-7c9c3c951d3a",
   "metadata": {},
   "source": [
    "#### Output\n",
    "This is just a standard linear layer, allowing any loss to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d2292-bf0b-4df7-bfa7-1ea20d1fe975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLinear(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d130f4f-54e5-4d4d-b9d6-6fcd1b5c2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, \n",
    "                 tgt_embed: InputEmbeddings, src_pos: PositionalEmbeddings, tgt_pos: PositionalEmbeddings, \n",
    "                 last_linear: LastLinear):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.last_linear = last_linear   #self-explanatory\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, enc_out, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, enc_out, tgt_mask, src_mask)\n",
    "\n",
    "    def linear(self, x):\n",
    "        return self.last_linear(x)\n",
    "    #this whole thing is made very nice by the classes defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120297a-4d19-40be-86a0-9dfccbfe82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len:int, d_model: int = 512, n_layers: int = 6,\n",
    "                      n_heads:int = 8, dropout: float = 0.1, hidden_size: int = 2048):\n",
    "    #first step is to make embedding layers:\n",
    "    src_embeddings = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embeddings = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    #now we make pos embed layers:\n",
    "    src_pos = PositionalEmbeddings(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEmbeddings(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    #create the encoder blocks:\n",
    "    encoder_blocks = []\n",
    "    for _ in range(n_layers):\n",
    "        encoder_self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        feed_forward = FeedForwardBlock(d_model, hidden_size, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention, feed_forward, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    #create the decoder blocks:\n",
    "    decoder_blocks = []\n",
    "    for _ in range(n_layers):\n",
    "        decoder_self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        decoder_cross_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        feed_forward = FeedForwardBlock(d_model, hidden_size, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention, decoder_cross_attention, feed_forward, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    #create encoder:\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "\n",
    "    #create decoder:\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    #create last linear layers\n",
    "    last_layer = LastLinear(d_model, tgt_vocab_size)\n",
    "\n",
    "    #finally time to define the Transformer in full:\n",
    "    transformer = Transformer(encoder, decoder, src_embeddings, tgt_embeddings, src_pos, tgt_pos, last_layer)\n",
    "\n",
    "    #initialise paramaters with Xavier intialisation (helps to avoid vanishing gradients)    \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)    \n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff10e39-7804-454b-af27-42d3e72506b4",
   "metadata": {},
   "source": [
    "### Training\n",
    "Before implementing a Music Transformer, I though it prudent to train this one to do machine translation as it was originally developed for by google. To do this I will use the multi30k dataset from torchtext, and pyTorch to implement tokenisation, batching etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92eb5a-ae7c-4239-bc1f-16305987de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from Transformer import build_transformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "\n",
    "#HYPERPARAMETERS\n",
    "max_src_len = 100\n",
    "max_tgt_len = 100\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "dropout = 0.1\n",
    "hidden_size = 2048\n",
    "batch_size = 32\n",
    "lr = 0.0001\n",
    "n_epochs = 10\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92592e98-e497-4021-87cd-71d98d55968b",
   "metadata": {},
   "source": [
    "First step is to load a dataset and a tokenizer. Here I'm using spaCy and the Multi30k dataset from torchtext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0acf0-1362-4456-b889-6e50e4390956",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')  #german tokenizer\n",
    "spacy_en = spacy.load('en_core_web_sm')     #english tokenizer\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]  #creates list of german tokens\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]  #creates list of english tokens\n",
    "\n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]  #symbols for unknows, padding, beginning of sequence, end of sequence\n",
    "\n",
    "#note we are translating from german to english so src is german and tgt is english\n",
    "def yield_tokens(data_iter, language):   #function to generate tokenized sentences in either language\n",
    "    for src_sample, tgt_sample in data_iter:\n",
    "        if language == 'de':\n",
    "            yield tokenize_de(src_sample)\n",
    "        else:\n",
    "            yield tokenize_en(tgt_sample)\n",
    "\n",
    "train_iter = Multi30k(split='train', language_pair=('de', 'en'))  #training data, note Multi30k returns tuples of (german, english) sentences\n",
    "src_vocab = build_vocab_from_iterator(yield_tokens(train_iter, 'de'), specials=special_symbols)  #builds german vocab\n",
    "src_vocab.set_default_index(src_vocab[\"<unk>\"])  #set default index for unknown words\n",
    "\n",
    "train_iter = Multi30k(split='train', language_pair=('de', 'en'))  #reset training data as it is an iterator\n",
    "tgt_vocab = build_vocab_from_iterator(yield_tokens(train_iter, 'en'), specials=special_symbols)  #builds english vocab\n",
    "tgt_vocab.set_default_index(tgt_vocab[\"<unk>\"])  #set default index for unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00853dd7-f7ae-45d2-94cb-572a857ec1f8",
   "metadata": {},
   "source": [
    "Now I create a custom collate function for the dataloader to use which will:\n",
    "- tokenize the sentences from the dataset (note these are already cleaned)\n",
    "- add <bos> and <eos> tokens\n",
    "- converts tokens to indices\n",
    "- pads the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ce0e8-4274-41f6-b67a-409bd06a7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_tokens =[\"<bos>\"] + tokenize_de(src_sample) + [\"<eos>\"]  #add <bos> and <eos> tokens\n",
    "        tgt_tokens =[\"<bos>\"] + tokenize_en(tgt_sample) + [\"<eos>\"]\n",
    "\n",
    "        src_tokens = src_tokens[:max_src_len]\n",
    "        tgt_tokens = tgt_tokens[:max_tgt_len]\n",
    "        \n",
    "        src_indices = src_vocab(src_tokens)  #convert tokens to indices\n",
    "        tgt_indices = tgt_vocab(tgt_tokens)\n",
    "\n",
    "        src_batch.append(torch.tensor(src_indices, dtype=torch.long))  #add to batch\n",
    "        tgt_batch.append(torch.tensor(tgt_indices, dtype=torch.long))\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab[\"<pad>\"])  #pad the sentences\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab[\"<pad>\"])\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4bfcf-d8c7-456f-8f93-4684323a6341",
   "metadata": {},
   "source": [
    "Now we create masks for the training data to be fed into the encoder and decoder blocks. In the encoder these will simply mask out any <pad> tokens to avoid error, and in the decoder we ensure casuality by masking out the top half of the matrix as explained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8847bb-7868-437b-b44f-17b7b0762d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(Multi30k(split='train', language_pair=('de', 'en')))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "#need to know when it starts padding in order to create masks for training\n",
    "src_pad_index = src_vocab['<pad>']\n",
    "tgt_pad_index = tgt_vocab['<pad>']\n",
    "\n",
    "def create_masks(src, tgt):\n",
    "    src_mask = (src != src_pad_index).unsqueeze(1).unsqueeze(2) #makes a matrix of 0s where there is padding, 1s elsewhere\n",
    "    tgt_pad_mask = (tgt != tgt_pad_index).unsqueeze(1) \n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    subseq_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device = device)).bool() #\"tril\" makes a lower triangular matrix, so this has 1s below the diagonal and 0s above to ensure casuality in MMA\n",
    "    tgt_mask = tgt_pad_mask & subseq_mask.unsqueeze(0) #takes into account padding also\n",
    "    tgt_mask = tgt_mask.unsqueeze(1)\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88417e33-655c-44b5-ab11-8064dfb624dd",
   "metadata": {},
   "source": [
    "I also implemented an evaluation function that will be run after each epoch to get another input as to how well the model is performing after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb18a58-d6a5-4eda-b5fd-77470e8be8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src, tgt = batch\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]  #remove <eos>\n",
    "            tgt_target = tgt[:, 1:]  #remove <bos>\n",
    "            src_mask, tgt_mask = create_masks(src, tgt_input)\n",
    "            enc_out = model.encode(src, src_mask)\n",
    "            dec_out = model.decode(enc_out, src_mask, tgt_input, tgt_mask)\n",
    "            output = model.linear(dec_out)\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_target.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    model.train()  #set back to training mode\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ce58c-e84b-4ec4-8bea-d1b626ef011c",
   "metadata": {},
   "source": [
    "All that's left is to define a standard training loop using Adam and CrossEntropyLoss in pyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4ffde-5839-4c2c-8367-8513311cab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = build_transformer(len(src_vocab), len(tgt_vocab), max_src_len, max_tgt_len, d_model, n_layers, n_heads, dropout, hidden_size)\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range (n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        src, tgt = batch\n",
    "        src = src.to(device)  #transformer expects shape (seq_len, batch_size)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]  #remove <eos> token\n",
    "        tgt_target = tgt[:, 1:]  #remove <bos> token\n",
    "        src_mask, tgt_mask = create_masks(src, tgt_input)\n",
    "        optimizer.zero_grad()\n",
    "        enc_out = net.encode(src, src_mask)\n",
    "        dec_out = net.decode(enc_out, src_mask,tgt_input, tgt_mask)\n",
    "        output = net.linear(dec_out)\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt_target.reshape(-1))  #reshape to 2D tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    valid_loss = evaluate(net, train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(train_dataloader)} Validation Loss: {valid_loss}\")\n",
    "\n",
    "torch.save(net.state_dict(), 'transformer_weights.pth') #save the weights for inference\n",
    "torch.save(src_vocab, 'src_vocab.pth')\n",
    "torch.save(tgt_vocab, 'tgt_vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e036c-22c3-4e09-9985-14f0b6a3b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1 Loss: 1.7731107343255226\n",
    "Epoch 2 Loss: 0.4258847597251259\n",
    "Epoch 3 Loss: 0.21841126423670018\n",
    "Epoch 4 Loss: 0.13542658144807343\n",
    "Epoch 5 Loss: 0.09036654730090232\n",
    "Epoch 6 Loss: 0.06228175041057342\n",
    "Epoch 7 Loss: 0.043085280171253584\n",
    "Epoch 8 Loss: 0.028818972463706197\n",
    "Epoch 9 Loss: 0.017904086781564542\n",
    "Epoch 10 Loss: 0.008983496685775229"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b0e8b-74e1-48ff-9398-d246006d4245",
   "metadata": {},
   "source": [
    "As you can see, even after just 10 epochs in a v small model (this only took about 5 minutes to train locally) it is already achieving very low loss, showcasing the power of the transformer architecture. Past 25 epochs there were no meaningful changes to epoch loss so I just left it at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a6f54-dff5-4ade-b308-e5cd7ebec0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 25 Loss: 4.1449397439095505e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c012a27-2479-4259-8369-ad3551cb6fa8",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "I wanted to run some tests on the trained model to get a feel for how well it ran, so I created a simple inference script to run inputs through the decoder of the trained model. Note the code below uses the same vocabularies as the training loop so I omitted that section here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc460b4-847d-4df0-96d6-e9c438fba999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transformer(len(src_vocab), len(tgt_vocab),\n",
    "                          max_src_len, max_tgt_len,\n",
    "                          d_model, n_layers, n_heads,\n",
    "                          dropout, hidden_size)\n",
    "model = model.to(device)\n",
    "\n",
    "#load saved weights after training\n",
    "model.load_state_dict(torch.load('transformer_weights.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "#translation function using beam search\n",
    "def translate_sentence(sentence, model, src_vocab, tgt_vocab, max_len=100, beam_size=5):\n",
    "    #preprocess input\n",
    "    tokens = [\"<bos>\"] + tokenize_de(sentence) + [\"<eos>\"]\n",
    "    src_indices = torch.tensor(src_vocab(tokens), dtype=torch.long).unsqueeze(0).to(device)  # [1, src_seq_len]\n",
    "    \n",
    "    src_mask = (src_indices != src_pad_index).unsqueeze(1).unsqueeze(2)\n",
    "    enc_out = model.encode(src_indices, src_mask)\n",
    "    \n",
    "    #define special tokens\n",
    "    bos_token = tgt_vocab[\"<bos>\"]\n",
    "    eos_token = tgt_vocab[\"<eos>\"]\n",
    "    \n",
    "    #initialize beam with a tuple (sequence, cumulative_log_prob)\n",
    "    beams = [([bos_token], 0.0)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        #if all beams already end with <eos>, stop expanding\n",
    "        if all(seq[-1] == eos_token for seq, score in beams):\n",
    "            break\n",
    "        \n",
    "        #expand each beam candidate\n",
    "        for seq, score in beams:\n",
    "            if seq[-1] == eos_token:\n",
    "                #do not expand if already ended\n",
    "                new_beams.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            #prepare target sequence tensor for the current beam\n",
    "            tgt_seq = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)  # [1, current_seq_len]\n",
    "            _, tgt_mask = create_masks(src_indices, tgt_seq)\n",
    "            dec_out = model.decode(enc_out, src_mask, tgt_seq, tgt_mask)\n",
    "            output = model.linear(dec_out)  # [1, current_seq_len, vocab_size]\n",
    "            \n",
    "            #consider only the last time step\n",
    "            token_logits = output[:, -1, :]  # [1, vocab_size]\n",
    "            log_probs = torch.log_softmax(token_logits, dim=-1).squeeze(0)  # [vocab_size]\n",
    "            \n",
    "            #get top beam_size token probabilities for current beam\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_size)\n",
    "            \n",
    "            #create new beam candidates by appending each top token\n",
    "            for k in range(beam_size):\n",
    "                new_seq = seq + [topk_indices[k].item()]\n",
    "                # Use a length penalty (alpha is a hyperparameter, e.g., 0.7):\n",
    "                length = len(seq) + 1  # +1 for the new token being added\n",
    "                penalty = (5 + length) ** alpha / (5 + 1) ** 0.7  # example from GNMT\n",
    "                new_score = (score + topk_log_probs[k].item()) / penalty\n",
    "                new_beams.append((new_seq, new_score))\n",
    "        \n",
    "        #keep only the top beam_size candidates across all expansions\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "    \n",
    "    #select the best candidate from the beams (highest score)\n",
    "    best_seq, best_score = beams[0]\n",
    "    #remove <bos> and trailing <eos> if present\n",
    "    if best_seq[0] == bos_token:\n",
    "        best_seq = best_seq[1:]\n",
    "    if best_seq and best_seq[-1] == eos_token:\n",
    "        best_seq = best_seq[:-1]\n",
    "    \n",
    "    #convert token indices to words using tgt_vocab.get_itos()\n",
    "    translated_tokens = [tgt_vocab.get_itos()[i] for i in best_seq]\n",
    "    return \" \".join(translated_tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_sentence = \"Ein junges Mädchen sitzt auf einer Bank und hält ein rotes Eis am Stiel.\"  #change this to any German sentence\n",
    "    translation = translate_sentence(input_sentence, model, src_vocab, tgt_vocab)\n",
    "    print(\"Input: \", input_sentence)\n",
    "    print(\"Translation: \", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e48208-c7a3-42ba-9d6c-1359d1ad7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input:  Ein junges Mädchen sitzt auf einer Bank und hält ein rotes Eis am Stiel.\n",
    "Translation:  pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool \n",
    "pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0102b-2f3a-42a1-a6a1-a8db5f0ce74c",
   "metadata": {},
   "source": [
    "Unfortunately I struggled greatly to get the model to output meaningful translations at this stage, likely due the exposure bias occuring because of use of teacher forcing in the training loop. Upscaling the model or using a larger dataset would probably also help, but these are not changes I'm interested in implementing at this time. The decreasing loss and validation loss is enough of a proof of concept for me at this point, I'd like to move on to the more interesting music generation project before investing time and resources into making it train and evaluate properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661ac44-6513-43c9-92c0-a7846fd764d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
